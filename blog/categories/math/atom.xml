<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Math | Blog 1]]></title>
  <link href="http://vincenttam.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://vincenttam.github.io/"/>
  <updated>2015-05-16T00:46:59+08:00</updated>
  <id>http://vincenttam.github.io/</id>
  <author>
    <name><![CDATA[Vincent Tam]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    
      <title type="html"><![CDATA[Found 2 Incomparable Topologies]]></title>
      <link href="http://vincenttam.github.io/blog/2015/04/25/found-2-incomparable-topologies/"/>
    
    <updated>2015-04-25T21:17:45+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/04/25/found-2-incomparable-topologies</id>
    
      <content type="html"><![CDATA[<ol>
  <li>
    <p>Lower limit topology ($\R_l$)</p>

    <ul>
      <li>Basis: $\{[a,b): a,b \in \R\text{ s.t. } a &lt; b\}$</li>
    </ul>
  </li>
  <li>
    <p>K-topology ($\R_K$)</p>

    <ul>
      <li>Basis: $\{(a,b), (a,b) - K: a,b \in \R\text{ s.t. } a &lt; b \}$,
 where $K = \{ 1/n : n \in \Z_+ \}$.</li>
    </ul>
  </li>
</ol>

<h2 id="rl-nsubseteq-rk">$\R_l \nsubseteq \R_K$</h2>

<p>Consider a base element $[a,b)$.  At the point $a$, <em>no</em> open interval
$(c,d)$ containing $a$ is a subset of $[a,b)$.</p>

<h2 id="rk-nsubseteq-rl">$\R_K \nsubseteq \R_l$</h2>

<p>Let $B_2 = (-1, 1) - K$.  At $B_2 \notin \R_l$ because any base
element $[0,b)$ containing 0 must hit $1/n$ for some $n \in \Z_+$ by
Archimedean Property of $\Z_+$.  Thus, $\forall b &gt; 0, [0,b)
\nsubseteq B_2$.</p>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Understood Euler Product Formula]]></title>
      <link href="http://vincenttam.github.io/blog/2015/04/12/understood-euler-product-formula/"/>
    
    <updated>2015-04-12T08:19:36+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/04/12/understood-euler-product-formula</id>
    
      <content type="html"><![CDATA[<h2 id="before-i-understood-this-formula">Before I understood this formula</h2>

<p>When I was a high school student, it’s <em>hard</em> for me to imagine a
product whose index looped through all prime numbers because primes
<em>don’t</em> appear in a regular way: between 1 and 100, there’re 25
primes, but between 900 and 1000, there’re 14.</p>

<p>Even though it’s easier to imagine the infinite sum whose $n$-th term
is $n^{-s}$, <em>without</em> learning the $p$-Test and the Comparison Test
for the convergence of infinite sums, I <em>couldn’t</em> understand why the
infinite sum in the following equality exists.</p>

<div class="myeqn">
\[
\sum_{k = 1}^{\infty} \frac{1}{k^s} = \prod_{p \text{ prime}}
\frac{1}{1 - p^{-s}}
\]
</div>

<h2 id="a-heuristic-way-to-understand-it">A heuristic way to understand it</h2>

<ol>
  <li>Note that the geometric series converge (absolutely).</li>
  <li>Borrow the proofs about the Comparison Test and the $p$-Test to
convince yourself that the infinite sum on the LHS of the formula is
<em>well-defined</em>.</li>
  <li>Know something about convergent infinite products.<sup id="fnref:wiki"><a href="#fn:wiki" class="footnote">1</a></sup></li>
  <li>Convince yourself that the infinite product on the RHS
<em>exists</em>.<sup id="fnref:inf_prod_exists"><a href="#fn:inf_prod_exists" class="footnote">2</a></sup></li>
  <li>Recall the Fundamental Theorem of Arithmetic.</li>
</ol>

<p>I think that the last item is the <em>trickiest</em> step.  Writing the
following lines, I understood this equation.</p>

<div class="myeqn">
\begin{align}
&amp; 1^{-s} + 2^{-s} + 3^{-s} + \cdots &#92;&#92;
=&amp; (1^{-s} + 3^{-s} + 5^{-s} + \cdots) (1^{-s} + 2^{-s} + 2^{-2s} +
\cdots) \label{step1} &#92;&#92;
=&amp; (1^{-s} + 3^{-s} + 5^{-s} + \cdots) \cdot \frac{1}{1 - 2^{-s}}
\label{step2} &#92;&#92;
=&amp; (1^{-s} + 5^{-s} + 7^{-s} + \cdots) (1^{-s} + 3^{-s} + 3^{-2s} +
\cdots) \cdot \frac{1}{1 - 2^{-s}} \label{step3} &#92;&#92;
=&amp; (1^{-s} + 5^{-s} + 7^{-s} + \cdots) \cdot \frac{1}{1 - 3^{-s}}
\cdot \frac{1}{1 - 2^{-s}} \label{step4}
\end{align}
</div>

<p>Steps \eqref{step1} (resp. \eqref{step3}) holds because for each
$k^{-s}$ in the leftmost bracket, powers of 2 (resp. 3) can be taken
out from $k$.  In steps \eqref{step2} and \eqref{step4}, the formula
for the sum of geometric series is applied.</p>

<h2 id="refining-the-above-thoughts">Refining the above thoughts</h2>

<p>I’ll end this post by wrapping up the above ideas by summation and
product signs.</p>

<div class="myeqn">
\[
\begin{aligned}
&amp; \sum_{k = 1}^{\infty} \frac{1}{k^s} &#92;&#92;
=&amp; \sum_{k \in \N} \frac{1}{k^s} &#92;&#92;
=&amp; \prod_{p \text{ prime}} \sum_{k \in \N} \frac{1}{p^{s k}} &#92;&#92;
=&amp; \prod_{p \text{ prime}} \sum_{k = 1}^{\infty}\frac{1}{p^{s k}}&#92;&#92;
=&amp; \prod_{p \text{ prime}} \frac{1}{1 - p^{-s}}
\end{aligned}
\]
</div>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:wiki">

      <p>One may refer to the convergence criteria of infinite products on
<a href="http://en.wikipedia.org/wiki/Infinite_product#Convergence_criteria">Wikipedia</a> <a href="#fnref:wiki" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:inf_prod_exists">

      <p>There’s more than one way to do it.  When I tried to do this for
the first time, I used the Mean Value Theorem on logarithms to
establish a standard result.</p>

      <div class="myeqn">
\[
\frac{x}{1 + x} &lt; \log(1 + x) &lt; x \quad \forall\, x &gt; 0
\]
</div>
      <p><a href="#fnref:inf_prod_exists" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Compared Two Poisson Variables]]></title>
      <link href="http://vincenttam.github.io/blog/2015/04/10/compared-two-poisson-variables/"/>
    
    <updated>2015-04-10T13:21:37+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/04/10/compared-two-poisson-variables</id>
    
      <content type="html"><![CDATA[<h2 id="background">Background</h2>

<p>Last Friday, I had to submit a homework which required me to evaluate
$\Pr(A &gt; B)$ and $\Pr(A = B)$, where $A$ and $B$ were two independent
Poisson random variables with parameters $\alpha$ and $\beta$
respectively.</p>

<h2 id="problem">Problem</h2>

<p>I then started evaluating the sum.</p>

<div class="myeqn">
\[
  \Pr(A &gt; B) = \sum_{i = 1}^\infty \sum_{j = 0}^{i - 1}
  \frac{e^{-\alpha} \alpha^i}{i!} \cdot \frac{e^{-\beta} \beta^j}{j!}
\]
</div>

<p>Then I was <em>stuck</em>.  I <em>couldn’t</em> compute this sum also.</p>

<div class="myeqn">
\[
  \Pr(A = B) = \sum_{i = 0}^\infty \frac{e^{-(\alpha + \beta)}
  \alpha^i \beta^i}{(i!)^2}
\]
</div>

<h2 id="fact">Fact</h2>

<p>I googled for a solution for hours, and after I saw equation (3.1) in
a paper, I gave up finding exact solutions.<sup id="fnref:fact"><a href="#fn:fact" class="footnote">1</a></sup>  As a supporter of
free software, I avoided using M$ Ex*, and wrote a program in C++ to
approximate the above probabitities by directly adding them term by
term.</p>

<h3 id="source-code">Source code</h3>

<p><div><script src='https://gist.github.com/c27c38c49fe8de17c815.js'></script>
<noscript><pre><code>#include &lt;iostream&gt;
#include &lt;cmath&gt;
using namespace std;

double pXy(double x, double y, int N);
double pxy(double x, double y, int N);

int main(void) {
    double pAb,paB,pab,a,b;
    int N;
    cout &lt;&lt; &quot;Assume that Poisson r.v. A and B are indepedent&quot; &lt;&lt; endl;
    cout &lt;&lt; &quot;Parameter for A: &quot;;
    cin &gt;&gt; a;
    cout &lt;&lt; &quot;Parameter for B: &quot;;
    cin &gt;&gt; b;
    cout &lt;&lt; &quot;Number of terms to be added (100 &lt;= N &lt;= 1000): &quot;;
    cin &gt;&gt; N;
    pAb = pXy(a,b,N);
    paB = pXy(b,a,N);
    pab = pxy(a,b,N);
    cout &lt;&lt; &quot;P(A &gt; B) = &quot; &lt;&lt; pAb &lt;&lt; &quot;, P(A &lt; B) = &quot; &lt;&lt; paB &lt;&lt;
        &quot;, P(A = B) = &quot; &lt;&lt; pab &lt;&lt; endl;
}

/* P(X &gt; Y) */
double pXy(double x, double y, int N) {
    double ans = 0;
    for (int i = 1; i &lt;= N; i++) {
        for (int j = 0; j &lt; i ; j++) {
            double term = 1;
            for (int k = 1; k &lt;= i; k++)
                term *= x / k;
            for (int k = 1; k &lt;= j; k++)
                term *= y / k;
            ans += term;
        }
    }
    return ans * exp(-x - y);
}

/* P(X = Y) */
double pxy(double x, double y, int N) {
    double ans = 0;
    for (int i = 0; i &lt;= N; i++) {
        double term = 1;
        for (int k = 1; k &lt;= i; k++)
            term *= x / k * y / k;
        ans += term;
    }
    return ans * exp(-x - y);
}
</code></pre></noscript></div>
</p>

<h3 id="sample-output">Sample output</h3>

<pre class="cliUB"><code>Assume that Poisson r.v. A and B are indepedent
Parameter for A: 1.6
Parameter for B: 1.4
Number of terms to be added (100 &lt;= N &lt;= 1000): 8
P(A &gt; B) = 0.423023, P(A &lt; B) = 0.335224, P(A = B) = 0.241691
</code></pre>

<h2 id="lessons-learnt">Lessons learnt</h2>

<ol>
  <li>
    <p>A one-line method for writing the content of a function which
returns the factorial of a number.</p>

    <p>URL: <a href="http://progopedia.com/example/factorial/">http://progopedia.com/example/factorial/</a></p>
  </li>
  <li>
    <p>Evaluation of a function inside GDB</p>

    <p>URL: <a href="http://stackoverflow.com/q/1354731/">http://stackoverflow.com/q/1354731/</a></p>
  </li>
</ol>

<hr />
<div class="footnotes">
  <ol>
    <li id="fn:fact">

      <p>Keller, J. B. (1994). A characterization of the Poisson
distribution and the probability of winning a game. <em>The American
Statistician</em>, 48(4), 294–298. <a href="#fnref:fact" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Calculating the Volume of a Triangular Pyramid in a Hard Way]]></title>
      <link href="http://vincenttam.github.io/blog/2015/04/07/calculating-the-volume-of-a-triangular-pyramid-in-a-hard-way/"/>
    
    <updated>2015-04-07T16:07:49+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/04/07/calculating-the-volume-of-a-triangular-pyramid-in-a-hard-way</id>
    
      <content type="html"><![CDATA[<h2 id="background">Background</h2>

<p>There is an easy way of calculating the volume of $\{(x,y,z) \in
\R^3: 0 \le x,y,z \le t, x + y + z \le t\}$: just consider the
permutation of $x,y,z$.<sup id="fnref:easy_vol"><a href="#fn:easy_vol" class="footnote">1</a></sup>  This can be easily generalized to
$n$ dimension.</p>

<h2 id="another-way-using-multiple-integrals">Another way using multiple integrals</h2>

<div class="myeqn">
\[
\begin{split}
&amp; \text{Let } A:= \left\{ (x_1,\ldots,x_n) \in \R^n: 0 \le x_i \le 1
\forall 1 \le i \le n, \sum\nolimits_{i = 1}^n x_i \le 1 \right\}.
&#92;&#92;
&amp; \text{Volume of } A &#92;&#92;
=&amp; \idotsint\limits_A \ud x_n \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 1} x_i} \ud x_n \cdots \ud x_3 \ud x_2
\ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 2} x_i} (t - \sum_{i = 1}^{n - 1} x_i)
\ud x_{n - 1} \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 2} x_i} x_{n - 1} \ud x_{n - 1} \cdots
\ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 3} x_i} \frac{(t - \sum_{i = 1}^{n -
2} x_i)^2}{2!} \ud x_{n - 2} \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 3} x_i} \frac{x_{n - 2}^2}{2!} \ud
x_{n - 2} \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 4} x_i} \frac{(t - \sum_{i = 1}^{n -
3} x_i)^3}{3!} \ud x_{n - 3} \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
&amp; \vdots &#92;&#92;
=&amp; \int_{0}^{t} \frac{(t - x_1)^{n - 1}}{(n - 1)!} \ud x_1 &#92;&#92;
=&amp; \frac{t^n}{n!}
\end{split}
\]
</div>

<h2 id="why-use-this-method">Why use this method?</h2>

<p>To calculate its centre of mass.</p>

<div class="myeqn">
\[
\begin{split}
&amp; \text{First component of its centre of mass} &#92;&#92;
=&amp; \frac{n!}{t^n} \idotsint\limits_A x_1 \ud x_n \cdots \ud x_3 \ud
x_2 \ud x_1 &#92;&#92;
=&amp; \frac{n!}{t^n} \int_{0}^{t} x_1 \int_{0}^{t - x_1} \int_{0}^{t -
x_1 - x_2} \cdots \int_{0}^{t - \sum_{i = 1}^{n - 1} x_i} \ud x_n
\cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \frac{n!}{t^n} \int_{0}^{t} x_1 \frac{(t - x_1)^{n - 1}}{(n - 1)!}
\ud x_1 &#92;&#92;
=&amp; \frac{n!}{t^n} \left( \left. -x_1 \frac{(t - x_1)^{n - 1}}{(n -
1)!} \right|_{0}^t + \int_{0}^{t} \frac{(t - x_1)^n}{n!} \ud x_1
\right) &#92;&#92;
=&amp; \frac{n!}{t^n} \left( 0 + \frac{t^{n + 1}}{(n + 1)!} \right) &#92;&#92;
=&amp; \frac{t}{n + 1}
\end{split}
\]
</div>

<p>By symmetry, we conclude that the center of mass is $(\frac{t}{n + 1},
\frac{t}{n + 1}, \ldots, \frac{t}{n + 1}) \in \R^n$.</p>

<hr />
<div class="footnotes">
  <ol>
    <li id="fn:easy_vol">

      <p>Simplex. (2015, March 29). In <em>Wikipedia, The Free Encyclopedia</em>.
Retrieved 15:34, April 6, 2015, from
<a href="http://en.wikipedia.org/w/index.php?title=Simplex&amp;oldid=654074423">http://en.wikipedia.org/w/index.php?title=Simplex&amp;oldid=654074423</a> <a href="#fnref:easy_vol" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Calculate the Expected Waiting Time in a Hard Way]]></title>
      <link href="http://vincenttam.github.io/blog/2015/04/06/calculate-the-expected-waiting-time-in-a-hard-way/"/>
    
    <updated>2015-04-06T22:18:03+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/04/06/calculate-the-expected-waiting-time-in-a-hard-way</id>
    
      <content type="html"><![CDATA[<h2 id="background">Background</h2>

<p>If one assumes that servers $X_1$ and $X_2$ has exponential service
times with rate $\lambda_1$ and $\lambda_2$ respectively, (i.e.
$X_i \sim \Exp(\lambda_i), i = 1,2$), then one can follow the
standard arguments and say that the waiting time $\min\left\{ X_1,
X_2 \right\} \sim \Exp(\lambda_1 + \lambda_2)$, so the expected
waiting time is $1/(\lambda_1 + \lambda_2)$.</p>

<h2 id="problem">Problem</h2>

<p>I tried finding the expected waiting time by conditioning on $X_1 - X_2$.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \Pr(X_1 &gt; X_2) &#92;&#92;
    =&amp; \int_{0}^{\infty} p_{X_1}(x_1) \Pr(X_2 &lt; x_1) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 e^{-\lambda_1 x_1} (1 -
    e^{-\lambda_2 x_1}) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 e^{-\lambda_1 x_1} \ud x_1 -
    \int_{0}^{\infty} \lambda_1 e^{-(\lambda_1 + \lambda_2) x_1} \ud
    x_1 &#92;&#92;
    =&amp; 1 + \left. \frac{\lambda_1}{\lambda_1 + \lambda_2}
    e^{-(\lambda_1 + \lambda_2) x_1} \right|_{0}^{\infty} &#92;&#92;
    =&amp; 1 - \frac{\lambda_1}{\lambda_1 + \lambda_2} &#92;&#92;
    =&amp; \frac{\lambda_2}{\lambda_1 + \lambda_2}
  \end{split}
  \label{eq:pr_min}
\end{equation}
</div>

<p>Similarly, one has $\Pr(X_1 \le X_2) = \lambda_1/(\lambda_1 +
\lambda_2)$.</p>

<div class="myeqn">
\begin{equation}
\begin{split}
&amp; \E[\left\{ X_1, X_2 \right\}] &#92;&#92;
=&amp; \E[\min\left\{ X_1, X_2 \right\} \mid X_1 &gt; X_2] \Pr(X_1 &gt; X_2)
&#92;&#92;
+&amp; \E[\min\left\{ X_1, X_2 \right\} \mid X_1 \le X_2] \Pr(X_1 \le X_2)
&#92;&#92;
=&amp; \E[X_2] \Pr(X_1 &gt; X_2) + \E[X_1] \Pr(X_1 \le X_2) &#92;&#92;
=&amp; \frac{1}{\lambda_2} \frac{\lambda_2}{\lambda_1 + \lambda_2} +
\frac{1}{\lambda_1} \frac{\lambda_1}{\lambda_2 + \lambda_1} &#92;&#92;
=&amp; \frac{2}{\lambda_1 + \lambda_2}
\end{split}
\label{eq:wrong}
\end{equation}
</div>

<p>This is <em>different</em> from what we expect.  <strong>What’s wrong with the
above calculation?</strong></p>

<h2 id="solution">Solution</h2>

<p>I really thought about the meaning of $\E[\min\left\{ X_1, X_2
\right\} \mid X_1 &gt; X_2]$, and find out that this conditional
expectation <em>won’t</em> be helpful because</p>

<div class="myeqn">
\[
  \E[\min\left\{ X_1, X_2 \right\} \mid X_1 &gt; X_2] =
  \frac{\int_{0}^{\infty} \int_{0}^{x_1} x_2 \ud x_2 \ud x_1}{\Pr(X_1
  &gt; X_2)}
\]
</div>

<p>Actually, one can divide it into two halves.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \E[\min\left\{ X_1, X_2 \right\}] &#92;&#92;
    =&amp; \int_{0}^{\infty} \int_{0}^{x_1} x_2 \lambda_1 \lambda_2
    e^{-\lambda_1 x_1 - \lambda_2 x_2} \ud x_2  \ud x_1 &#92;&#92;
    +&amp; \int_{0}^{\infty} \int_{0}^{x_2} x_1 \lambda_2 \lambda_1
	e^{-\lambda_2 x_2 - \lambda_1 x_1} \ud x_1  \ud x_2
  \end{split}
  \label{eq:head}
\end{equation}
</div>

<p>By observing the symmetry between the subscripts ‘1’ and ‘2’ in the
above equation, we only need to evaluate <em>one</em> of them.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \int_{0}^{\infty} \int_{0}^{x_1} x_2 \lambda_1 \lambda_2
    e^{-\lambda_1 x_1 - \lambda_2 x_2} \ud x_2  \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    \int_{0}^{x_1} x_2 e^{-\lambda_2 x_2} \ud x_2 \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    \left. -x_2 \cdot \frac{e^{-\lambda_2 x_2}}{\lambda_2}
    \right|_{x_2 = 0}^{x_2 = x_1} + \int_{0}^{x_1} \frac{e^{-\lambda_2
    x_2}}{\lambda_2} \ud x_2 \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    -x_1 \cdot \frac{e^{-\lambda_2 x_1}}{\lambda_2} - \left.
    \frac{e^{-\lambda_2 x_2}}{\lambda_2^2} \right|_{x_2 = 0}^{x_2 =
    x_1} \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    -x_1 \cdot \frac{e^{-\lambda_2 x_1}}{\lambda_2} + \frac{1 -
    e^{-\lambda_2 x_1}}{\lambda_2^2} \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} -\lambda_1 x_1 e^{-(\lambda_1 + \lambda_2)
    x_1} \ud x_1 + \int_{0}^{\infty} \frac{\lambda_1}{\lambda_2}
    (e^{-\lambda_1 x_1} - e^{-(\lambda_1 + \lambda_2) x_1}) \ud x_1
    &#92;&#92;
    =&amp; \lambda_1 \left( \left. \frac{x_1 e^{-(\lambda_1 + \lambda_2)
    x_1}}{\lambda_1 + \lambda_2} \right|_{0}^{\infty} -
    \int_{0}^{\infty} \frac{e^{-(\lambda_1 + \lambda_2)
    x_1}}{\lambda_1 + \lambda_2} \right) &#92;&#92;
    +&amp; \frac{\lambda_1}{\lambda_2} \left( \left. -\frac{e^{\lambda_1
    x_1}}{\lambda_1} \right|_{0}^{\infty} + \left.
    \frac{e^{-(\lambda_1 + \lambda_2) x_1}}{\lambda_1 + \lambda_2}
    \right|_{0}^{\infty} \right) &#92;&#92;
    =&amp; \lambda_1 \left( 0 + \left. \frac{e^{-(\lambda_1 + \lambda_2)
    x_1}}{(\lambda_1 + \lambda_2)^2} \right|_{0}^{\infty} \right) +
    \frac{\lambda_1}{\lambda_2} \left( \frac{1}{\lambda_1} -
    \frac{1}{\lambda_1 + \lambda_2} \right) &#92;&#92;
    =&amp; -\frac{\lambda_1}{(\lambda_1 + \lambda_2)^2} +
    \frac{1}{\lambda_2} - \frac{\lambda_1}{\lambda_2 (\lambda_1 +
    \lambda_2)}
  \end{split}
  \label{eq:half_int}
\end{equation}
</div>

<p>Similarly, one has</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \int_{0}^{\infty} \int_{0}^{x_2} x_1 \lambda_2 \lambda_1
    e^{-\lambda_2 x_2 - \lambda_1 x_1} \ud x_1  \ud x_2 &#92;&#92;
    =&amp; -\frac{\lambda_2}{(\lambda_2 + \lambda_1)^2} +
    \frac{1}{\lambda_1} - \frac{\lambda_2}{\lambda_1 (\lambda_2 +
    \lambda_1)}.
  \end{split}
  \label{eq:half_int2}
\end{equation}
</div>

<p>Substitute \eqref{eq:half_int} and \eqref{eq:half_int2} into
\eqref{eq:head}.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \E[\min\left\{ X_1, X_2 \right\}] &#92;&#92;
    =&amp; \left( -\frac{\lambda_1}{(\lambda_1 + \lambda_2)^2} +
    \frac{1}{\lambda_2} - \frac{\lambda_1}{\lambda_2 (\lambda_1 +
    \lambda_2)} \right) &#92;&#92;
    +&amp; \left( -\frac{\lambda_2}{(\lambda_2 + \lambda_1)^2} +
    \frac{1}{\lambda_1} - \frac{\lambda_2}{\lambda_1 (\lambda_2 +
    \lambda_1)} \right) &#92;&#92;
    =&amp; -\left( \frac{\lambda_1}{(\lambda_2 + \lambda_1)^2} +
    \frac{\lambda_2}{(\lambda_2 + \lambda_1)^2} \right) + \left(
    \frac{1}{\lambda_1} + \frac{1}{\lambda_2} \right) &#92;&#92;
    -&amp; \left( \frac{\lambda_1}{\lambda_2 (\lambda_1 + \lambda_2)} +
    \frac{\lambda_2}{\lambda_1 (\lambda_2 + \lambda_1)} \right) &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{\lambda_1 +
    \lambda_2}{\lambda_1 \lambda_2} - \frac{\lambda_1^2 +
    \lambda_2^2}{\lambda_1 \lambda_2 (\lambda_1 + \lambda_2)} &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{(\lambda_1 +
    \lambda_2)^2 - (\lambda_1^2 + \lambda_2^2)}{\lambda_1 \lambda_2
    (\lambda_1 + \lambda_2)} &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{2\lambda_1
    \lambda_2}{\lambda_1 \lambda_2 (\lambda_1 + \lambda_2)} &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{2}{\lambda_1 +
    \lambda_2} &#92;&#92;
    =&amp; \frac{1}{\lambda_1 + \lambda_2}
  \end{split}
  \label{eq:fin}
\end{equation}
</div>

<p>This is consistent with what we expect.  I finally understand what’s
wrong in \eqref{eq:wrong}: $X_1$ <em>isn’t</em> independent from $X_1 -
X_2$.</p>

<h2 id="generalization">Generalization</h2>

<p>By induction, we can generalize \eqref{eq:fin} to the expected waiting
time for $n$ servers in parallel: if $X_i \sim \Exp(\lambda_i)
\forall 1 \le i \le n$, then</p>

<div class="myeqn">
\[
  \E[\min\left\{ X_1, \ldots, X_n \right\}] = \frac{1}{\sum_{k = 1}^n
  \lambda_k}.
\]
</div>
]]></content>
    
  </entry>
  
</feed>
