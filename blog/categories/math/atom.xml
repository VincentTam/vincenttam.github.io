<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Math | Blog 1]]></title>
  <link href="http://vincenttam.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://vincenttam.github.io/"/>
  <updated>2015-04-10T14:46:42+08:00</updated>
  <id>http://vincenttam.github.io/</id>
  <author>
    <name><![CDATA[Vincent Tam]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    
      <title type="html"><![CDATA[Compared Two Poisson Variables]]></title>
      <link href="http://vincenttam.github.io/blog/2015/04/10/compared-two-poisson-variables/"/>
    
    <updated>2015-04-10T13:21:37+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/04/10/compared-two-poisson-variables</id>
    
      <content type="html"><![CDATA[<h2 id="background">Background</h2>

<p>Last Friday, I had to submit a homework which required me to evaluate
$\Pr(A &gt; B)$ and $\Pr(A = B)$, where $A$ and $B$ were two independent
Poisson random variables with parameters $\alpha$ and $\beta$
respectively.</p>

<h2 id="problem">Problem</h2>

<p>I then started evaluating the sum.</p>

<div class="myeqn">
\[
  \Pr(A &gt; B) = \sum_{i = 1}^\infty \sum_{j = 0}^{i - 1}
  \frac{e^{-\alpha} \alpha^i}{i!} \cdot \frac{e^{-\beta} \beta^j}{j!}
\]
</div>

<p>Then I was <em>stuck</em>.  I <em>couldn’t</em> compute this sum also.</p>

<div class="myeqn">
\[
  \Pr(A = B) = \sum_{i = 0}^\infty \frac{e^{-(\alpha + \beta)}
  \alpha^i \beta^i}{(i!)^2}
\]
</div>

<h2 id="fact">Fact</h2>

<p>I googled for a solution for hours, and after I saw equation (3.1) in
a paper, I gave up finding exact solutions.<sup id="fnref:fact"><a href="#fn:fact" class="footnote">1</a></sup>  As a supporter of
free software, I avoided using M$ Ex*, and wrote a program in C++ to
approximate the above probabitities by directly adding them term by
term.</p>

<h3 id="source-code">Source code</h3>

<p><div><script src='https://gist.github.com/c27c38c49fe8de17c815.js'></script>
<noscript><pre><code>#include &lt;iostream&gt;
#include &lt;cmath&gt;
using namespace std;

double pXy(double x, double y, int N);
double pxy(double x, double y, int N);

int main(void) {
    double pAb,paB,pab,a,b;
    int N;
    cout &lt;&lt; &quot;Assume that Poisson r.v. A and B are indepedent&quot; &lt;&lt; endl;
    cout &lt;&lt; &quot;Parameter for A: &quot;;
    cin &gt;&gt; a;
    cout &lt;&lt; &quot;Parameter for B: &quot;;
    cin &gt;&gt; b;
    cout &lt;&lt; &quot;Number of terms to be added (100 &lt;= N &lt;= 1000): &quot;;
    cin &gt;&gt; N;
    pAb = pXy(a,b,N);
    paB = pXy(b,a,N);
    pab = pxy(a,b,N);
    cout &lt;&lt; &quot;P(A &gt; B) = &quot; &lt;&lt; pAb &lt;&lt; &quot;, P(A &lt; B) = &quot; &lt;&lt; paB &lt;&lt;
        &quot;, P(A = B) = &quot; &lt;&lt; pab &lt;&lt; endl;
}

/* P(X &gt; Y) */
double pXy(double x, double y, int N) {
    double ans = 0;
    for (int i = 1; i &lt;= N; i++) {
        for (int j = 0; j &lt; i ; j++) {
            double term = 1;
            for (int k = 1; k &lt;= i; k++)
                term *= x / k;
            for (int k = 1; k &lt;= j; k++)
                term *= y / k;
            ans += term;
        }
    }
    return ans * exp(-x - y);
}

/* P(X = Y) */
double pxy(double x, double y, int N) {
    double ans = 0;
    for (int i = 0; i &lt;= N; i++) {
        double term = 1;
        for (int k = 1; k &lt;= i; k++)
            term *= x / k * y / k;
        ans += term;
    }
    return ans * exp(-x - y);
}
</code></pre></noscript></div>
</p>

<h3 id="sample-output">Sample output</h3>

<pre class="cliUB"><code>Assume that Poisson r.v. A and B are indepedent
Parameter for A: 1.6
Parameter for B: 1.4
Number of terms to be added (100 &lt;= N &lt;= 1000): 8
P(A &gt; B) = 0.423023, P(A &lt; B) = 0.335224, P(A = B) = 0.241691
</code></pre>

<h2 id="lessons-learnt">Lessons learnt</h2>

<ol>
  <li>
    <p>A one-line method for writing the content of a function which
returns the factorial of a number.</p>

    <p>URL: <a href="http://progopedia.com/example/factorial/">http://progopedia.com/example/factorial/</a></p>
  </li>
  <li>
    <p>Evaluation of a function inside GDB</p>

    <p>URL: <a href="http://stackoverflow.com/q/1354731/">http://stackoverflow.com/q/1354731/</a></p>
  </li>
</ol>

<hr />
<div class="footnotes">
  <ol>
    <li id="fn:fact">

      <p>Keller, J. B. (1994). A characterization of the Poisson
distribution and the probability of winning a game. <em>The American
Statistician</em>, 48(4), 294–298. <a href="#fnref:fact" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Calculating the Volume of a Triangular Pyramid in a Hard Way]]></title>
      <link href="http://vincenttam.github.io/blog/2015/04/07/calculating-the-volume-of-a-triangular-pyramid-in-a-hard-way/"/>
    
    <updated>2015-04-07T16:07:49+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/04/07/calculating-the-volume-of-a-triangular-pyramid-in-a-hard-way</id>
    
      <content type="html"><![CDATA[<h2 id="background">Background</h2>

<p>There is an easy way of calculating the volume of $\{(x,y,z) \in
\R^3: 0 \le x,y,z \le t, x + y + z \le t\}$: just consider the
permutation of $x,y,z$.<sup id="fnref:easy_vol"><a href="#fn:easy_vol" class="footnote">1</a></sup>  This can be easily generalized to
$n$ dimension.</p>

<h2 id="another-way-using-multiple-integrals">Another way using multiple integrals</h2>

<div class="myeqn">
\[
\begin{split}
&amp; \text{Let } A:= \left\{ (x_1,\ldots,x_n) \in \R^n: 0 \le x_i \le 1
\forall 1 \le i \le n, \sum\nolimits_{i = 1}^n x_i \le 1 \right\}.
&#92;&#92;
&amp; \text{Volume of } A &#92;&#92;
=&amp; \idotsint\limits_A \ud x_n \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 1} x_i} \ud x_n \cdots \ud x_3 \ud x_2
\ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 2} x_i} (t - \sum_{i = 1}^{n - 1} x_i)
\ud x_{n - 1} \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 2} x_i} x_{n - 1} \ud x_{n - 1} \cdots
\ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 3} x_i} \frac{(t - \sum_{i = 1}^{n -
2} x_i)^2}{2!} \ud x_{n - 2} \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 3} x_i} \frac{x_{n - 2}^2}{2!} \ud
x_{n - 2} \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \int_{0}^{t} \int_{0}^{t - x_1} \int_{0}^{t - x_1 - x_2} \cdots
\int_{0}^{t - \sum_{i = 1}^{n - 4} x_i} \frac{(t - \sum_{i = 1}^{n -
3} x_i)^3}{3!} \ud x_{n - 3} \cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
&amp; \vdots &#92;&#92;
=&amp; \int_{0}^{t} \frac{(t - x_1)^{n - 1}}{(n - 1)!} \ud x_1 &#92;&#92;
=&amp; \frac{t^n}{n!}
\end{split}
\]
</div>

<h2 id="why-use-this-method">Why use this method?</h2>

<p>To calculate its centre of mass.</p>

<div class="myeqn">
\[
\begin{split}
&amp; \text{First component of its centre of mass} &#92;&#92;
=&amp; \frac{n!}{t^n} \idotsint\limits_A x_1 \ud x_n \cdots \ud x_3 \ud
x_2 \ud x_1 &#92;&#92;
=&amp; \frac{n!}{t^n} \int_{0}^{t} x_1 \int_{0}^{t - x_1} \int_{0}^{t -
x_1 - x_2} \cdots \int_{0}^{t - \sum_{i = 1}^{n - 1} x_i} \ud x_n
\cdots \ud x_3 \ud x_2 \ud x_1 &#92;&#92;
=&amp; \frac{n!}{t^n} \int_{0}^{t} x_1 \frac{(t - x_1)^{n - 1}}{(n - 1)!}
\ud x_1 &#92;&#92;
=&amp; \frac{n!}{t^n} \left( \left. -x_1 \frac{(t - x_1)^{n - 1}}{(n -
1)!} \right|_{0}^t + \int_{0}^{t} \frac{(t - x_1)^n}{n!} \ud x_1
\right) &#92;&#92;
=&amp; \frac{n!}{t^n} \left( 0 + \frac{t^{n + 1}}{(n + 1)!} \right) &#92;&#92;
=&amp; \frac{t}{n + 1}
\end{split}
\]
</div>

<p>By symmetry, we conclude that the center of mass is $(\frac{t}{n + 1},
\frac{t}{n + 1}, \ldots, \frac{t}{n + 1}) \in \R^n$.</p>

<hr />
<div class="footnotes">
  <ol>
    <li id="fn:easy_vol">

      <p>Simplex. (2015, March 29). In <em>Wikipedia, The Free Encyclopedia</em>.
Retrieved 15:34, April 6, 2015, from
<a href="http://en.wikipedia.org/w/index.php?title=Simplex&amp;oldid=654074423">http://en.wikipedia.org/w/index.php?title=Simplex&amp;oldid=654074423</a> <a href="#fnref:easy_vol" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Calculate the Expected Waiting Time in a Hard Way]]></title>
      <link href="http://vincenttam.github.io/blog/2015/04/06/calculate-the-expected-waiting-time-in-a-hard-way/"/>
    
    <updated>2015-04-06T22:18:03+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/04/06/calculate-the-expected-waiting-time-in-a-hard-way</id>
    
      <content type="html"><![CDATA[<h2 id="background">Background</h2>

<p>If one assumes that servers $X_1$ and $X_2$ has exponential service
times with rate $\lambda_1$ and $\lambda_2$ respectively, (i.e.
$X_i \sim \Exp(\lambda_i), i = 1,2$), then one can follow the
standard arguments and say that the waiting time $\min\left\{ X_1,
X_2 \right\} \sim \Exp(\lambda_1 + \lambda_2)$, so the expected
waiting time is $1/(\lambda_1 + \lambda_2)$.</p>

<h2 id="problem">Problem</h2>

<p>I tried finding the expected waiting time by conditioning on $X_1 - X_2$.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \Pr(X_1 &gt; X_2) &#92;&#92;
    =&amp; \int_{0}^{\infty} p_{X_1}(x_1) \Pr(X_2 &lt; x_1) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 e^{-\lambda_1 x_1} (1 -
    e^{-\lambda_2 x_1}) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 e^{-\lambda_1 x_1} \ud x_1 -
    \int_{0}^{\infty} \lambda_1 e^{-(\lambda_1 + \lambda_2) x_1} \ud
    x_1 &#92;&#92;
    =&amp; 1 + \left. \frac{\lambda_1}{\lambda_1 + \lambda_2}
    e^{-(\lambda_1 + \lambda_2) x_1} \right|_{0}^{\infty} &#92;&#92;
    =&amp; 1 - \frac{\lambda_1}{\lambda_1 + \lambda_2} &#92;&#92;
    =&amp; \frac{\lambda_2}{\lambda_1 + \lambda_2}
  \end{split}
  \label{eq:pr_min}
\end{equation}
</div>

<p>Similarly, one has $\Pr(X_1 \le X_2) = \lambda_1/(\lambda_1 +
\lambda_2)$.</p>

<div class="myeqn">
\begin{equation}
\begin{split}
&amp; \E[\left\{ X_1, X_2 \right\}] &#92;&#92;
=&amp; \E[\min\left\{ X_1, X_2 \right\} \mid X_1 &gt; X_2] \Pr(X_1 &gt; X_2)
&#92;&#92;
+&amp; \E[\min\left\{ X_1, X_2 \right\} \mid X_1 \le X_2] \Pr(X_1 \le X_2)
&#92;&#92;
=&amp; \E[X_2] \Pr(X_1 &gt; X_2) + \E[X_1] \Pr(X_1 \le X_2) &#92;&#92;
=&amp; \frac{1}{\lambda_2} \frac{\lambda_2}{\lambda_1 + \lambda_2} +
\frac{1}{\lambda_1} \frac{\lambda_1}{\lambda_2 + \lambda_1} &#92;&#92;
=&amp; \frac{2}{\lambda_1 + \lambda_2}
\end{split}
\label{eq:wrong}
\end{equation}
</div>

<p>This is <em>different</em> from what we expect.  <strong>What’s wrong with the
above calculation?</strong></p>

<h2 id="solution">Solution</h2>

<p>I really thought about the meaning of $\E[\min\left\{ X_1, X_2
\right\} \mid X_1 &gt; X_2]$, and find out that this conditional
expectation <em>won’t</em> be helpful because</p>

<div class="myeqn">
\[
  \E[\min\left\{ X_1, X_2 \right\} \mid X_1 &gt; X_2] =
  \frac{\int_{0}^{\infty} \int_{0}^{x_1} x_2 \ud x_2 \ud x_1}{\Pr(X_1
  &gt; X_2)}
\]
</div>

<p>Actually, one can divide it into two halves.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \E[\min\left\{ X_1, X_2 \right\}] &#92;&#92;
    =&amp; \int_{0}^{\infty} \int_{0}^{x_1} x_2 \lambda_1 \lambda_2
    e^{-\lambda_1 x_1 - \lambda_2 x_2} \ud x_2  \ud x_1 &#92;&#92;
    +&amp; \int_{0}^{\infty} \int_{0}^{x_2} x_1 \lambda_2 \lambda_1
	e^{-\lambda_2 x_2 - \lambda_1 x_1} \ud x_1  \ud x_2
  \end{split}
  \label{eq:head}
\end{equation}
</div>

<p>By observing the symmetry between the subscripts ‘1’ and ‘2’ in the
above equation, we only need to evaluate <em>one</em> of them.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \int_{0}^{\infty} \int_{0}^{x_1} x_2 \lambda_1 \lambda_2
    e^{-\lambda_1 x_1 - \lambda_2 x_2} \ud x_2  \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    \int_{0}^{x_1} x_2 e^{-\lambda_2 x_2} \ud x_2 \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    \left. -x_2 \cdot \frac{e^{-\lambda_2 x_2}}{\lambda_2}
    \right|_{x_2 = 0}^{x_2 = x_1} + \int_{0}^{x_1} \frac{e^{-\lambda_2
    x_2}}{\lambda_2} \ud x_2 \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    -x_1 \cdot \frac{e^{-\lambda_2 x_1}}{\lambda_2} - \left.
    \frac{e^{-\lambda_2 x_2}}{\lambda_2^2} \right|_{x_2 = 0}^{x_2 =
    x_1} \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    -x_1 \cdot \frac{e^{-\lambda_2 x_1}}{\lambda_2} + \frac{1 -
    e^{-\lambda_2 x_1}}{\lambda_2^2} \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} -\lambda_1 x_1 e^{-(\lambda_1 + \lambda_2)
    x_1} \ud x_1 + \int_{0}^{\infty} \frac{\lambda_1}{\lambda_2}
    (e^{-\lambda_1 x_1} - e^{-(\lambda_1 + \lambda_2) x_1}) \ud x_1
    &#92;&#92;
    =&amp; \lambda_1 \left( \left. \frac{x_1 e^{-(\lambda_1 + \lambda_2)
    x_1}}{\lambda_1 + \lambda_2} \right|_{0}^{\infty} -
    \int_{0}^{\infty} \frac{e^{-(\lambda_1 + \lambda_2)
    x_1}}{\lambda_1 + \lambda_2} \right) &#92;&#92;
    +&amp; \frac{\lambda_1}{\lambda_2} \left( \left. -\frac{e^{\lambda_1
    x_1}}{\lambda_1} \right|_{0}^{\infty} + \left.
    \frac{e^{-(\lambda_1 + \lambda_2) x_1}}{\lambda_1 + \lambda_2}
    \right|_{0}^{\infty} \right) &#92;&#92;
    =&amp; \lambda_1 \left( 0 + \left. \frac{e^{-(\lambda_1 + \lambda_2)
    x_1}}{(\lambda_1 + \lambda_2)^2} \right|_{0}^{\infty} \right) +
    \frac{\lambda_1}{\lambda_2} \left( \frac{1}{\lambda_1} -
    \frac{1}{\lambda_1 + \lambda_2} \right) &#92;&#92;
    =&amp; -\frac{\lambda_1}{(\lambda_1 + \lambda_2)^2} +
    \frac{1}{\lambda_2} - \frac{\lambda_1}{\lambda_2 (\lambda_1 +
    \lambda_2)}
  \end{split}
  \label{eq:half_int}
\end{equation}
</div>

<p>Similarly, one has</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \int_{0}^{\infty} \int_{0}^{x_2} x_1 \lambda_2 \lambda_1
    e^{-\lambda_2 x_2 - \lambda_1 x_1} \ud x_1  \ud x_2 &#92;&#92;
    =&amp; -\frac{\lambda_2}{(\lambda_2 + \lambda_1)^2} +
    \frac{1}{\lambda_1} - \frac{\lambda_2}{\lambda_1 (\lambda_2 +
    \lambda_1)}.
  \end{split}
  \label{eq:half_int2}
\end{equation}
</div>

<p>Substitute \eqref{eq:half_int} and \eqref{eq:half_int2} into
\eqref{eq:head}.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \E[\min\left\{ X_1, X_2 \right\}] &#92;&#92;
    =&amp; \left( -\frac{\lambda_1}{(\lambda_1 + \lambda_2)^2} +
    \frac{1}{\lambda_2} - \frac{\lambda_1}{\lambda_2 (\lambda_1 +
    \lambda_2)} \right) &#92;&#92;
    +&amp; \left( -\frac{\lambda_2}{(\lambda_2 + \lambda_1)^2} +
    \frac{1}{\lambda_1} - \frac{\lambda_2}{\lambda_1 (\lambda_2 +
    \lambda_1)} \right) &#92;&#92;
    =&amp; -\left( \frac{\lambda_1}{(\lambda_2 + \lambda_1)^2} +
    \frac{\lambda_2}{(\lambda_2 + \lambda_1)^2} \right) + \left(
    \frac{1}{\lambda_1} + \frac{1}{\lambda_2} \right) &#92;&#92;
    -&amp; \left( \frac{\lambda_1}{\lambda_2 (\lambda_1 + \lambda_2)} +
    \frac{\lambda_2}{\lambda_1 (\lambda_2 + \lambda_1)} \right) &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{\lambda_1 +
    \lambda_2}{\lambda_1 \lambda_2} - \frac{\lambda_1^2 +
    \lambda_2^2}{\lambda_1 \lambda_2 (\lambda_1 + \lambda_2)} &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{(\lambda_1 +
    \lambda_2)^2 - (\lambda_1^2 + \lambda_2^2)}{\lambda_1 \lambda_2
    (\lambda_1 + \lambda_2)} &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{2\lambda_1
    \lambda_2}{\lambda_1 \lambda_2 (\lambda_1 + \lambda_2)} &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{2}{\lambda_1 +
    \lambda_2} &#92;&#92;
    =&amp; \frac{1}{\lambda_1 + \lambda_2}
  \end{split}
  \label{eq:fin}
\end{equation}
</div>

<p>This is consistent with what we expect.  I finally understand what’s
wrong in \eqref{eq:wrong}: $X_1$ <em>isn’t</em> independent from $X_1 -
X_2$.</p>

<h2 id="generalization">Generalization</h2>

<p>By induction, we can generalize \eqref{eq:fin} to the expected waiting
time for $n$ servers in parallel: if $X_i \sim \Exp(\lambda_i)
\forall 1 \le i \le n$, then</p>

<div class="myeqn">
\[
  \E[\min\left\{ X_1, \ldots, X_n \right\}] = \frac{1}{\sum_{k = 1}^n
  \lambda_k}.
\]
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Two Diagrams Illustrating the Isomorphism Extension Theorem]]></title>
      <link href="http://vincenttam.github.io/blog/2015/03/28/two-diagrams-illustrating-the-isomorphism-extension-theorem/"/>
    
    <updated>2015-03-28T17:33:58+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/03/28/two-diagrams-illustrating-the-isomorphism-extension-theorem</id>
    
      <content type="html"><![CDATA[<p>Two weeks ago, I had proved that any two algebraic closures of a field
are isomorphic to each other in a homework problem.  To finish this
problem, I opened my note book to view the diagram for the Isomorphism
Extension Theorem (<abbr title="Isomorphism Extension Theorem">IET</abbr>) drawn before I had understood the proof of the
existence of algebraic closure.<sup id="fnref:pp-eac"><a href="#fn:pp-eac" class="footnote">1</a></sup></p>

<object type="image/svg+xml" data="/downloads/code/svgpan_1.2.2/IET.svg" width="300" height="300">
  Your browser does not support SVG
</object>

<p><small>Drag the figure to translate it, and scroll to enlarge/reduce
it.<sup id="fnref:tech"><a href="#fn:tech" class="footnote">2</a></sup><br />
Source code: <a href="/downloads/code/IET.tex">$\rm \LaTeX$</a>, <a href="/downloads/code/svgpan_1.2.2/IET.svg">SVG</a></small></p>

<p>After I had read E. Artin’s construction of an algebraic closure of a
field, I had also read the proof of <abbr title="Isomorphism Extension Theorem">IET</abbr>.<sup id="fnref:pp-artin"><a href="#fn:pp-artin" class="footnote">3</a></sup>  After that, I
thought I understood this theorem.  However, I <em>couldn’t</em> figure out
how to make use of the above diagram to do this question.</p>

<div class="myeqn">
\begin{equation*}\begin{CD}
     @.              \overline{F&#8217;}&#92;&#92;
@.                   @AAA&#92;&#92;
E    @&gt;\tau&gt;\cong&gt;   \tau[E]&#92;&#92;
@AAA                 @AAA&#92;&#92;
F    @&gt;\sigma&gt;\cong&gt; F&#8217;
\end{CD}\end{equation*}
</div>

<p><small>I <em>can’t</em> add dashed arrow for $\tau$.</small></p>

<p>I then opened John B. Fraleigh’s <em>A First Course in Abstract Algebra</em>
and saw two diagrams which illustrated the <abbr title="Isomorphism Extension Theorem">IET</abbr>.  In those two figures,
there’re only vertical and horizontal lines, <em>no</em> oblique lines were
found.</p>

<p>Using these diagrams, I successfully answered this question by drawing
a tower of five levels of algebraic extensions.</p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:pp-eac">

      <p><a href="/blog/2015/02/21/read-a-proof-of-existence-of-algebraic-closure/"><em>Read a Proof of Existence of Algebraic Closure</em></a> on Blog 1. <a href="#fnref:pp-eac" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tech">

      <p>When it comes to drawing commutative diagrams, MathJax only
supports AMScd, which <em>doesn’t</em> support diagonal arrows.
Therefore, I used <code>tikz-cd</code> according to the last sentence of
<a href="http://www.jmilne.org/not/CDGuide.html"><em>Guide to Commutative Diagram Packages</em></a> by J.S. Milne
to produce a standalone diagram in PDF format first.  Then I
converted it to an SVG file using the procedures described in the
last paragraph in <a href="/blog/2014/06/21/export-pdf-to-svg/" title="Export PDF to SVG">my earlier post about pdf2svg</a> on Blog 1.
Finally, I added the dragging and scrolling features to the SVG
files after re-reading <a href="/blog/2014/08/02/zooming-svg-in-web-browsers/"><em>Zooming SVG in Web Browsers</em></a> on
Blog 1. <a href="#fnref:tech" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pp-artin">

      <p>Same as footnote 1. <a href="#fnref:pp-artin" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[A Non-surjective Embedding Mapping a Field to Itself]]></title>
      <link href="http://vincenttam.github.io/blog/2015/03/28/a-non-surjective-embedding-mapping-a-field-to-itself/"/>
    
    <updated>2015-03-28T13:38:52+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/03/28/a-non-surjective-embedding-mapping-a-field-to-itself</id>
    
      <content type="html"><![CDATA[<p>A week ago, I came up with an injective, but <em>not surjective</em>
homomorphism which mapped a field to the same field: $\phi: \Q(e) \to
\Q(e)$ defined by $\phi(e) = e^2$ and $\left.\phi\right|_\Q =
\id_{\Q}$.  It <em>isn’t</em> surjective because $\phi[\Q(e)] = \Q(e^2)
\subsetneq \Q(e)$</p>

<p>Obviously, this kind of mapping <em>wasn’t</em> defined on a finite field.</p>

<p>After that, I found another non-surjective embedding which sends field
$\Z_p [y]$, where $y$ is an indeterminate, to itself on Mathematics
Stack Exchange.<sup id="fnref:mathse91688"><a href="#fn:mathse91688" class="footnote">1</a></sup></p>

<p>From this, I’ve understood that why the Isomorphism Extension Theomrem
<em>doesn’t</em> apply to transcendental field extensions.</p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:mathse91688">

      <p><a href="http://math.stackexchange.com/q/91688"><em>How to prove that the Frobenius homomorphism is surjective?</em></a>
on Mathematics Stack Exchange. <a href="#fnref:mathse91688" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
</feed>
