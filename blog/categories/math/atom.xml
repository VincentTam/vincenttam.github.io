<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Math | Blog 1]]></title>
  <link href="http://vincenttam.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://vincenttam.github.io/"/>
  <updated>2015-04-06T22:39:25+08:00</updated>
  <id>http://vincenttam.github.io/</id>
  <author>
    <name><![CDATA[Vincent Tam]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    
      <title type="html"><![CDATA[Calculate the Expected Waiting Time in a Hard Way]]></title>
      <link href="http://vincenttam.github.io/blog/2015/04/06/calculate-the-expected-waiting-time-in-a-hard-way/"/>
    
    <updated>2015-04-06T22:18:03+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/04/06/calculate-the-expected-waiting-time-in-a-hard-way</id>
    
      <content type="html"><![CDATA[<h2 id="background">Background</h2>

<p>If one assumes that servers $X_1$ and $X_2$ has exponential service
times with rate $\lambda_1$ and $\lambda_2$ respectively, (i.e.
$X_i \sim \Exp(\lambda_i), i = 1,2$), then one can follow the
standard arguments and say that the waiting time $\min\left\{ X_1,
X_2 \right\} \sim \Exp(\lambda_1 + \lambda_2)$, so the expected
waiting time is $1/(\lambda_1 + \lambda_2)$.</p>

<h2 id="problem">Problem</h2>

<p>I tried finding the expected waiting time by conditioning on $X_1 - X_2$.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \Pr(X_1 &gt; X_2) &#92;&#92;
    =&amp; \int_{0}^{\infty} p_{X_1}(x_1) \Pr(X_2 &lt; x_1) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 e^{-\lambda_1 x_1} (1 -
    e^{-\lambda_2 x_1}) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 e^{-\lambda_1 x_1} \ud x_1 -
    \int_{0}^{\infty} \lambda_1 e^{-(\lambda_1 + \lambda_2) x_1} \ud
    x_1 &#92;&#92;
    =&amp; 1 + \left. \frac{\lambda_1}{\lambda_1 + \lambda_2}
    e^{-(\lambda_1 + \lambda_2) x_1} \right|_{0}^{\infty} &#92;&#92;
    =&amp; 1 - \frac{\lambda_1}{\lambda_1 + \lambda_2} &#92;&#92;
    =&amp; \frac{\lambda_2}{\lambda_1 + \lambda_2}
  \end{split}
  \label{eq:pr_min}
\end{equation}
</div>

<p>Similarly, one has $\Pr(X_1 \le X_2) = \lambda_1/(\lambda_1 +
\lambda_2)$.</p>

<div class="myeqn">
\begin{equation}
\begin{split}
&amp; \E[\left\{ X_1, X_2 \right\}] &#92;&#92;
=&amp; \E[\min\left\{ X_1, X_2 \right\} \mid X_1 &gt; X_2] \Pr(X_1 &gt; X_2) +
\E[\min\left\{ X_1, X_2 \right\} \mid X_1 \le X_2] \Pr(X_1 \le X_2)
&#92;&#92;
=&amp; \E[X_2] \Pr(X_1 &gt; X_2) + \E[X_1] \Pr(X_1 \le X_2) &#92;&#92;
=&amp; \frac{1}{\lambda_2} \frac{\lambda_2}{\lambda_1 + \lambda_2} +
\frac{1}{\lambda_1} \frac{\lambda_1}{\lambda_2 + \lambda_1} &#92;&#92;
=&amp; \frac{2}{\lambda_1 + \lambda_2}
\end{split}
\label{eq:wrong}
\end{equation}
</div>

<p>This is <em>different</em> from what we expect.  <strong>What’s wrong with the
above calculation?</strong></p>

<h2 id="solution">Solution</h2>

<p>I really thought about the meaning of $\E[\min\left\{ X_1, X_2
\right\} \mid X_1 &gt; X_2]$, and find out that this conditional
expectation <em>won’t</em> be helpful because</p>

<div class="myeqn">
\[
  \E[\min\left\{ X_1, X_2 \right\} \mid X_1 &gt; X_2] =
  \frac{\int_{0}^{\infty} \int_{0}^{x_1} x_2 \ud x_2 \ud x_1}{\Pr(X_1
  &gt; X_2)}
\]
</div>

<p>Actually, one can divide it into two halves.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \E[\min\left\{ X_1, X_2 \right\}] &#92;&#92;
    =&amp; \int_{0}^{\infty} \int_{0}^{x_1} x_2 \lambda_1 \lambda_2
    e^{-\lambda_1 x_1 - \lambda_2 x_2} \ud x_2  \ud x_1
    + \int_{0}^{\infty} \int_{0}^{x_2} x_1 \lambda_2 \lambda_1
	e^{-\lambda_2 x_2 - \lambda_1 x_1} \ud x_1  \ud x_2
  \end{split}
  \label{eq:head}
\end{equation}
</div>

<p>By observing the symmetry between the subscripts ‘1’ and ‘2’ in the
above equation, we only need to evaluate <em>one</em> of them.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \int_{0}^{\infty} \int_{0}^{x_1} x_2 \lambda_1 \lambda_2
    e^{-\lambda_1 x_1 - \lambda_2 x_2} \ud x_2  \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    \int_{0}^{x_1} x_2 e^{-\lambda_2 x_2} \ud x_2 \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    \left. -x_2 \cdot \frac{e^{-\lambda_2 x_2}}{\lambda_2}
    \right|_{x_2 = 0}^{x_2 = x_1} + \int_{0}^{x_1} \frac{e^{-\lambda_2
    x_2}}{\lambda_2} \ud x_2 \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    -x_1 \cdot \frac{e^{-\lambda_2 x_1}}{\lambda_2} - \left.
    \frac{e^{-\lambda_2 x_2}}{\lambda_2^2} \right|_{x_2 = 0}^{x_2 =
    x_1} \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} \lambda_1 \lambda_2 e^{-\lambda_1 x_1} \left(
    -x_1 \cdot \frac{e^{-\lambda_2 x_1}}{\lambda_2} + \frac{1 -
    e^{-\lambda_2 x_1}}{\lambda_2^2} \right) \ud x_1 &#92;&#92;
    =&amp; \int_{0}^{\infty} -\lambda_1 x_1 e^{-(\lambda_1 + \lambda_2)
    x_1} \ud x_1 + \int_{0}^{\infty} \frac{\lambda_1}{\lambda_2}
    (e^{-\lambda_1 x_1} - e^{-(\lambda_1 + \lambda_2) x_1}) \ud x_1
    &#92;&#92;
    =&amp; \lambda_1 \left( \left. \frac{x_1 e^{-(\lambda_1 + \lambda_2)
    x_1}}{\lambda_1 + \lambda_2} \right|_{0}^{\infty} -
    \int_{0}^{\infty} \frac{e^{-(\lambda_1 + \lambda_2)
    x_1}}{\lambda_1 + \lambda_2} \right) + \frac{\lambda_1}{\lambda_2}
    \left( \left. -\frac{e^{\lambda_1 x_1}}{\lambda_1}
    \right|_{0}^{\infty} + \left. \frac{e^{-(\lambda_1 + \lambda_2)
    x_1}}{\lambda_1 + \lambda_2} \right|_{0}^{\infty} \right) &#92;&#92;
    =&amp; \lambda_1 \left( 0 + \left. \frac{e^{-(\lambda_1 + \lambda_2)
    x_1}}{(\lambda_1 + \lambda_2)^2} \right|_{0}^{\infty} \right) +
    \frac{\lambda_1}{\lambda_2} \left( \frac{1}{\lambda_1} -
    \frac{1}{\lambda_1 + \lambda_2} \right) &#92;&#92;
    =&amp; -\frac{\lambda_1}{(\lambda_1 + \lambda_2)^2} +
    \frac{1}{\lambda_2} - \frac{\lambda_1}{\lambda_2 (\lambda_1 +
    \lambda_2)}
  \end{split}
  \label{eq:half_int}
\end{equation}
</div>

<p>Similarly, one has</p>

<div class="myeqn">
\begin{equation}
  \int_{0}^{\infty} \int_{0}^{x_2} x_1 \lambda_2 \lambda_1
  e^{-\lambda_2 x_2 - \lambda_1 x_1} \ud x_1  \ud x_2 =
  -\frac{\lambda_2}{(\lambda_2 + \lambda_1)^2} + \frac{1}{\lambda_1} -
  \frac{\lambda_2}{\lambda_1 (\lambda_2 + \lambda_1)}.
  \label{eq:half_int2}
\end{equation}
</div>

<p>Substitute \eqref{eq:half_int} and \eqref{eq:half_int2} into
\eqref{eq:head}.</p>

<div class="myeqn">
\begin{equation}
  \begin{split}
    &amp; \E[\min\left\{ X_1, X_2 \right\}] &#92;&#92;
    =&amp; \left( -\frac{\lambda_1}{(\lambda_1 + \lambda_2)^2} +
    \frac{1}{\lambda_2} - \frac{\lambda_1}{\lambda_2 (\lambda_1 +
    \lambda_2)} \right)
    + \left( -\frac{\lambda_2}{(\lambda_2 + \lambda_1)^2} +
	\frac{1}{\lambda_1} - \frac{\lambda_2}{\lambda_1 (\lambda_2 +
	\lambda_1)} \right) &#92;&#92;
    =&amp; -\left( \frac{\lambda_1}{(\lambda_2 + \lambda_1)^2} +
    \frac{\lambda_2}{(\lambda_2 + \lambda_1)^2} \right) + \left(
    \frac{1}{\lambda_1} + \frac{1}{\lambda_2} \right) - \left(
    \frac{\lambda_1}{\lambda_2 (\lambda_1 + \lambda_2)} +
    \frac{\lambda_2}{\lambda_1 (\lambda_2 + \lambda_1)} \right) &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{\lambda_1 +
    \lambda_2}{\lambda_1 \lambda_2} - \frac{\lambda_1^2 +
    \lambda_2^2}{\lambda_1 \lambda_2 (\lambda_1 + \lambda_2)} &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{(\lambda_1 +
    \lambda_2)^2 - (\lambda_1^2 + \lambda_2^2)}{\lambda_1 \lambda_2
    (\lambda_1 + \lambda_2)} &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{2\lambda_1
    \lambda_2}{\lambda_1 \lambda_2 (\lambda_1 + \lambda_2)} &#92;&#92;
    =&amp; -\frac{1}{\lambda_1 + \lambda_2} + \frac{2}{\lambda_1 +
    \lambda_2} &#92;&#92;
    =&amp; \frac{1}{\lambda_1 + \lambda_2}
  \end{split}
  \label{eq:fin}
\end{equation}
</div>

<p>This is consistent with what we expect.  I finally understand what’s
wrong in \eqref{eq:wrong}: $X_1$ <em>isn’t</em> independent from $X_1 -
X_2$.</p>

<h2 id="generalization">Generalization</h2>

<p>By induction, we can generalize \eqref{eq:fin} to the expected waiting
time for $n$ servers in parallel: if $X_i \sim \Exp(\lambda_i)
\forall 1 \le i \le n$, then</p>

<div class="myeqn">
\[
  \E[\min\left\{ X_1, \ldots, X_n \right\}] = \frac{1}{\sum_{k = 1}^n
  \lambda_k}.
\]
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Two Diagrams Illustrating the Isomorphism Extension Theorem]]></title>
      <link href="http://vincenttam.github.io/blog/2015/03/28/two-diagrams-illustrating-the-isomorphism-extension-theorem/"/>
    
    <updated>2015-03-28T17:33:58+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/03/28/two-diagrams-illustrating-the-isomorphism-extension-theorem</id>
    
      <content type="html"><![CDATA[<p>Two weeks ago, I had proved that any two algebraic closures of a field
are isomorphic to each other in a homework problem.  To finish this
problem, I opened my note book to view the diagram for the Isomorphism
Extension Theorem (<abbr title="Isomorphism Extension Theorem">IET</abbr>) drawn before I had understood the proof of the
existence of algebraic closure.<sup id="fnref:pp-eac"><a href="#fn:pp-eac" class="footnote">1</a></sup></p>

<object type="image/svg+xml" data="/downloads/code/svgpan_1.2.2/IET.svg" width="300" height="300">
  Your browser does not support SVG
</object>

<p><small>Drag the figure to translate it, and scroll to enlarge/reduce
it.<sup id="fnref:tech"><a href="#fn:tech" class="footnote">2</a></sup><br />
Source code: <a href="/downloads/code/IET.tex">$\rm \LaTeX$</a>, <a href="/downloads/code/svgpan_1.2.2/IET.svg">SVG</a></small></p>

<p>After I had read E. Artin’s construction of an algebraic closure of a
field, I had also read the proof of <abbr title="Isomorphism Extension Theorem">IET</abbr>.<sup id="fnref:pp-artin"><a href="#fn:pp-artin" class="footnote">3</a></sup>  After that, I
thought I understood this theorem.  However, I <em>couldn’t</em> figure out
how to make use of the above diagram to do this question.</p>

<div class="myeqn">
\begin{equation*}\begin{CD}
     @.              \overline{F&#8217;}&#92;&#92;
@.                   @AAA&#92;&#92;
E    @&gt;\tau&gt;\cong&gt;   \tau[E]&#92;&#92;
@AAA                 @AAA&#92;&#92;
F    @&gt;\sigma&gt;\cong&gt; F&#8217;
\end{CD}\end{equation*}
</div>

<p><small>I <em>can’t</em> add dashed arrow for $\tau$.</small></p>

<p>I then opened John B. Fraleigh’s <em>A First Course in Abstract Algebra</em>
and saw two diagrams which illustrated the <abbr title="Isomorphism Extension Theorem">IET</abbr>.  In those two figures,
there’re only vertical and horizontal lines, <em>no</em> oblique lines were
found.</p>

<p>Using these diagrams, I successfully answered this question by drawing
a tower of five levels of algebraic extensions.</p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:pp-eac">

      <p><a href="/blog/2015/02/21/read-a-proof-of-existence-of-algebraic-closure/"><em>Read a Proof of Existence of Algebraic Closure</em></a> on Blog 1. <a href="#fnref:pp-eac" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tech">

      <p>When it comes to drawing commutative diagrams, MathJax only
supports AMScd, which <em>doesn’t</em> support diagonal arrows.
Therefore, I used <code>tikz-cd</code> according to the last sentence of
<a href="http://www.jmilne.org/not/CDGuide.html"><em>Guide to Commutative Diagram Packages</em></a> by J.S. Milne
to produce a standalone diagram in PDF format first.  Then I
converted it to an SVG file using the procedures described in the
last paragraph in <a href="/blog/2014/06/21/export-pdf-to-svg/" title="Export PDF to SVG">my earlier post about pdf2svg</a> on Blog 1.
Finally, I added the dragging and scrolling features to the SVG
files after re-reading <a href="/blog/2014/08/02/zooming-svg-in-web-browsers/"><em>Zooming SVG in Web Browsers</em></a> on
Blog 1. <a href="#fnref:tech" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pp-artin">

      <p>Same as footnote 1. <a href="#fnref:pp-artin" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[A Non-surjective Embedding Mapping a Field to Itself]]></title>
      <link href="http://vincenttam.github.io/blog/2015/03/28/a-non-surjective-embedding-mapping-a-field-to-itself/"/>
    
    <updated>2015-03-28T13:38:52+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/03/28/a-non-surjective-embedding-mapping-a-field-to-itself</id>
    
      <content type="html"><![CDATA[<p>A week ago, I came up with an injective, but <em>not surjective</em>
homomorphism which mapped a field to the same field: $\phi: \Q(e) \to
\Q(e)$ defined by $\phi(e) = e^2$ and $\left.\phi\right|_\Q =
\id_{\Q}$.  It <em>isn’t</em> surjective because $\phi[\Q(e)] = \Q(e^2)
\subsetneq \Q(e)$</p>

<p>Obviously, this kind of mapping <em>wasn’t</em> defined on a finite field.</p>

<p>After that, I found another non-surjective embedding which sends field
$\Z_p [y]$, where $y$ is an indeterminate, to itself on Mathematics
Stack Exchange.<sup id="fnref:mathse91688"><a href="#fn:mathse91688" class="footnote">1</a></sup></p>

<p>From this, I’ve understood that why the Isomorphism Extension Theomrem
<em>doesn’t</em> apply to transcendental field extensions.</p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:mathse91688">

      <p><a href="http://math.stackexchange.com/q/91688"><em>How to prove that the Frobenius homomorphism is surjective?</em></a>
on Mathematics Stack Exchange. <a href="#fnref:mathse91688" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Powers of Roots of Irreducible Polynomials in a Field With Characteristic $p$]]></title>
      <link href="http://vincenttam.github.io/blog/2015/03/15/powers-of-roots-of-irreducible-polynomials-in-a-field-with-characteristic-p/"/>
    
    <updated>2015-03-15T13:53:12+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/03/15/powers-of-roots-of-irreducible-polynomials-in-a-field-with-characteristic-p</id>
    
      <content type="html"><![CDATA[<p>Suppose we have a field $F$ of characteristic $p$ and a degree $n$
irreducible polynomial $f \in F[x]$.</p>

<div class="myeqn">
\[
f(x) = \sum_{i = 0}^n a_i x^i \text{, where } a_i \in F \,\forall i =
0,1,\dots,n.
\]
</div>

<p>One can find a root $\alpha \notin F$ of $f$ in a field extension $E$
of $F$ by Kronecker’s Theorem.  Then $f(\alpha) = 0$. <strong>How about the
other $n - 1$ roots of $f$?</strong></p>

<p>Half a month ago, I <em>couldn’t</em> find out the answer directly — I used
the fact that all finite fields of order $p^n$ were isomorphic to
$\F_{p^n}$, which was the collection of roots of $x^{p^n} - x$ in
$\Z_p$ in $\overline{\Z_p}$.  However, this makes use of <em>too many</em>
abstract facts.</p>

<p>Yesterday night, by computing the $p$-th power of an element $\beta$
in $E$, I finally know the direct way of finding the other $n - 1$
roots of $f$ in $E$.  Let $b_0,\dots,b_n \in F$ such that</p>

<div class="myeqn">
\[
\beta = \sum_{i = 0}^{n - 1} b_i \alpha^i.
\]
</div>

<p>Compute the $p$-th power of $\beta$.</p>

<div class="myeqn">
\[
\beta^p = (\sum_{i = 0}^{n - 1} b_i \alpha^i)^p = \sum_{k_0,\dots,k_{n
- 1}} \frac{p!}{\prod_{i = 0}^{n - 1} k_i!} \prod_{i = 0}^{n - 1} (b_i
  \alpha^i)^{k_i},
\]
</div>

<div class="myeqn">
\begin{equation}
\text{ where } 0 \le k_i \le p \,\forall i = 0,\dots,n \text{ and }
\sum_{i = 0}^{n - 1} k_i = p. \label{eq:cond}
\end{equation}
</div>

<p>Case 1: $\exists k_j = p$, then $k_i = 0 \,\forall i \ne j$.</p>

<div class="myeqn">
\[
\frac{p!}{\prod_{i = 0}^{n - 1} k_i!} \prod_{i = 0}^{n - 1} (b_i
\alpha^i)^{k_i} = 1 \cdot (b_j \alpha^j)^{k_j} = b_j \alpha^{pj}
\]
</div>

<p>Note that one can prove that $\forall b \in F, b^p = b$ by induction.</p>

<p>Case 2: $k_i \ne p \,\forall i = 0,\dots,n$.  Since $p$ is a prime,</p>

<div class="myeqn">
\[
\frac{p!}{\prod_{i = 0}^{n - 1} k_i!} = 0 \text{ in } \Z_p
\]
</div>

<p>Since one only has $n$ choices of $k_0,\dots,k_{n - 1}$ which
satisfy \eqref{eq:cond}, we conclude that</p>

<div class="myeqn">
\begin{equation}
\beta^p = (\sum_{i = 0}^{n - 1} b_i \alpha^i)^p = \sum_{i = 0}^{n - 1}
b_i^p \alpha^{pi} \label{eq:powp}
\end{equation}
</div>

<p>Since $\alpha$ is a root of $f$ in $E$, $(f(\alpha))^p = 0$.
Replacing “$n - 1$” by “$n$” in the derivation of \eqref{eq:powp}, one
gets</p>

<div class="myeqn">
\[
(f(\alpha))^p = (\sum_{i = 0}^n a_i \alpha^i)^p = \sum_{i = 0}^n a_i
\alpha^{pi} = f(\alpha^p) = 0
\]
</div>

<p>Therefore, <em>without</em> learning induction, one can sense that
$f(\alpha^{p^m}) = 0 \,\forall m \in \N$.  That’s not the end.  Since
the degree of $f$ is $n$, we expect to that the number of roots of $f$
is <em>finite</em>.  Since we expect $n$ roots of $f$, we hope that
$\alpha^{p^m}$ will repeat itself for sufficiently large $m$.  This
hope comes true due to Lagrange’s Theorem — $|F^\times| = p^n -
1$, so $\alpha^{p^n - 1} = 1$.</p>

<p><del>
Unluckily, I’ve just found out that $f$ <em>has</em> some root $\alpha’$
which <em>doesn’t</em> hit either one of
$\alpha,\alpha^p,\alpha^{p^2},\dots,\alpha^{p^{n - 1}}$.  For example,
if one sets $f(x) := x^{p^2} - x \in \Z_p[x]$ and let $\alpha$ be a
root of $f$ in $\overline{\Z_p}$, then it’s trivial that
$\alpha^{p^2} = \alpha$, thus the collection of the $p^m$-th power of
$\alpha$ is just $\left\{ \alpha,\alpha^p \right\}$.  (i.e.
$\left\{ \alpha^{p^m} \mid m \in \N \right\} = \left\{
\alpha,\alpha^p \right\}$)  Nevertheless, $f$ should have $p^2$ roots
in $\overline{\Z_p}$.
</del></p>

<p>Hence, I <em>didn’t</em> succeed in answering <strong>the above bolded question</strong>,
but I still learn something about the roots of an irreducible
polynomial in an algebraic extension.</p>

<hr />
<p>(Edited on MAR 28, 2015)</p>

<p>With the results from perfect fields, we <em>immediately</em> know that $f$
has $n$ different roots.</p>
]]></content>
    
  </entry>
  
  <entry>
    
      <title type="html"><![CDATA[Learnt Cauchy--Bunyakovsky--Schwarz Inequality for Definite Integrals]]></title>
      <link href="http://vincenttam.github.io/blog/2015/02/28/learnt-cauchy-bunyakovsky-schwarz-inequality-for-definite-integrals/"/>
    
    <updated>2015-02-28T22:37:59+08:00</updated>
    <id>http://vincenttam.github.io/blog/2015/02/28/learnt-cauchy-bunyakovsky-schwarz-inequality-for-definite-integrals</id>
    
      <content type="html"><![CDATA[<p>When I was a secondary school student, I was quite satisfied with this
proof of Cauchy–Schwarz Inequality.</p>

<div class="myeqn">
\[
\begin{aligned}
&amp; (\sum_{i = 1}^n a_i b_i)^2 \le (\sum_{i = 1}^n a_i^2)(\sum_{j = 1}^n
b_j^2) &#92;&#92;
\iff&amp; (\sum_{i = 1}^n a_i b_i) (\sum_{j = 1}^n a_j b_j) \le (\sum_{i =
1}^n a_i^2)(\sum_{j = 1}^n b_j^2) &#92;&#92;
\iff&amp; \sum_{i = 1}^n \sum_{j = 1}^n a_i a_j b_i b_j \le \sum_{i = 1}^n
\sum_{j = 1}^n a_i^2 b_j^2 &#92;&#92;
\iff&amp; \sum_{i = 1}^n \sum_{j = 1}^n a_i a_j b_i b_j \le \sum_{i = 1}^n
\sum_{j = 1}^n \frac{1}{2} \left(a_i^2 b_j^2 + a_j^2 b_i^2 \right)&#92;&#92;
\iff&amp; \sum_{i = 1}^n \sum_{j = 1}^n \frac{1}{2} (a_i^2 b_j^2 - 2 a_i
b_j \cdot a_j b_i + a_j^2 b_i^2) \ge 0 &#92;&#92;
\iff&amp; \sum_{i = 1}^n \sum_{j = 1}^n \frac{1}{2} (a_i b_j - a_j b_i)^2
\ge 0
\end{aligned}
\]
</div>

<p>Equality holds if and only if <span class="myeqn">$\forall i,j = 1,\dots,n, a_i b_j - a_j b_i = 0$</span>.
I was so happy that I <em>didn’t</em> think of further generalisations.  I
<em>didn’t</em> realise that the inequality can be more concisely written as
$\langle \vect{a}, \vect{b} \rangle^2 \le \norm{\vect{a}}^2
\norm{\vect{b}}^2$, where <span class="myeqn">$\vect{a} =
(a_1,\cdots,a_n), \vect{b} = (b_1,\cdots,b_n) \in \R^n$</span></p>

<p>This Friday evening, I did a question about the integral version of
the inequality.  After spending hours to come up with an idea, I
realised why I needed to learn inner product spaces.  The very first
version of the inequality that I learnt has too many summation signs,
and it <em>can’t</em> be easily generalised to other spaces.  The second
proof of the <em>same</em> inequality that I learnt makes use of the
determinant of a quadratic polynomial $p(t) = \norm{\vect{u} - t
\vect{v}}^2$.  That proof is much more elegant, and it helped me a lot
while I was doing that question.  When equality holds, it’s very hard
to imagine what happens by looking at the original equality.  However,
if we convert it into the determinant of $p(t)$, then one quickly
knows that this is equivalent to $p(t) = 0$, and can easily conclude
that equality holds if and only if the two functions $f$ and $g$
satisfy $f \equiv k g$ for some $k \in \R$ for almost all
points.<sup id="fnref:rmk"><a href="#fn:rmk" class="footnote">1</a></sup></p>

<hr />
<div class="footnotes">
  <ol>
    <li id="fn:rmk">

      <p>To be more precise, if one accepts that the space $C([a,b])$ of
continuous functions defined on a closed and bounded interval
$[a,b]$ in an inner product space, and $f,g \in C([a,b])$,
then the condition “for almost all points” can be dropped.  If we
<em>don’t</em> to want be to so strict on the functions $f$ and $g$
defined on $[a,b]$, and we just say that $f$ and $g$ are
integrable functions defined on $[a,b]$, then we have to accept
the fact that $\displaystyle \int_a^b f^2 = 0$ <em>doesn’t</em> imply
that $f \equiv 0$. <a href="#fnref:rmk" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
    
  </entry>
  
</feed>
